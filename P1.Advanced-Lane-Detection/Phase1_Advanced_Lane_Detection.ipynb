{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Phase1_Advanced_Lane_Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmed192a/ImageProcessing-PatternRecognition/blob/main/P1.Advanced-Lane-Detection/Phase1_Advanced_Lane_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase one: Lane Detection\n",
        "**In this first phase**, the goal is to write a software pipeline to identify the lane boundaries in a video from a front-facing camera on a car. itâ€™s required to find and track the lane lines and the\n",
        "position of the car from the center of the lane. <br>\n",
        "As a bonus, track the radius of curvature of the road too.\n",
        "\n",
        "Assume the camera is mounted at the center of the car, such that the lane center is the midpoint at the bottom of the image between the two lines you've detected. \n",
        "\n",
        "The offset of the lane center from the center of the image (converted from pixels to meters) is your distance from the center of the lane."
      ],
      "metadata": {
        "id": "2uxC7E_vZLi7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "krgTM9UXamaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from moviepy.editor import VideoFileClip\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode"
      ],
      "metadata": {
        "id": "E9_2f4OBdeP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Line Class\n"
      ],
      "metadata": {
        "id": "DPJY1o_hbR-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LaneLines:\n",
        "    def __init__(self):\n",
        "        self.left_fit = None\n",
        "        self.right_fit = None\n",
        "        self.binary = None\n",
        "        self.nonzero = None\n",
        "        self.nonzerox = None\n",
        "        self.nonzeroy = None\n",
        "        self.clear_visibility = True\n",
        "        self.dir = []\n",
        "    \n",
        "        # HYPERPARAMETERS\n",
        "        # Number of sliding windows\n",
        "        self.nwindows = 9\n",
        "        # Width of the the windows +/- margin\n",
        "        self.margin = 100\n",
        "        # Mininum number of pixels found to recenter window\n",
        "        self.minpix = 50\n",
        "    \n",
        "    def forward(self, img):\n",
        "        self.extract_features(img)\n",
        "        return self.fit_poly(img)\n",
        "\n",
        "    def fit_poly(self, img):\n",
        "        out = np.dstack((img, img, img))\n",
        "        leftx, lefty, rightx, righty, out_img = self.find_lane_pixels(img)\n",
        "\n",
        "        if len(lefty) > 1500:\n",
        "            self.left_fit = np.polyfit(lefty, leftx, 2)\n",
        "        if len(righty) > 1500:\n",
        "            self.right_fit = np.polyfit(righty, rightx, 2)\n",
        "\n",
        "        # Generate x and y values for plotting\n",
        "        maxy = img.shape[0] - 1\n",
        "        miny = img.shape[0] // 3\n",
        "        if len(lefty):\n",
        "            maxy = max(maxy, np.max(lefty))\n",
        "            miny = min(miny, np.min(lefty))\n",
        "\n",
        "        if len(righty):\n",
        "            maxy = max(maxy, np.max(righty))\n",
        "            miny = min(miny, np.min(righty))\n",
        "\n",
        "        ploty = np.linspace(miny, maxy, img.shape[0])\n",
        "\n",
        "        left_fitx = self.left_fit[0]*ploty**2 + self.left_fit[1]*ploty + self.left_fit[2]\n",
        "        right_fitx = self.right_fit[0]*ploty**2 + self.right_fit[1]*ploty + self.right_fit[2]\n",
        "\n",
        "        # Visualization\n",
        "        c = 0\n",
        "        for i, y in enumerate(ploty):\n",
        "            c = c+1\n",
        "            if(c == 2):\n",
        "                yo = int(y)\n",
        "                lo = int(l)\n",
        "                ro = int(r)\n",
        "            y = int(ploty[i])\n",
        "            l = int(left_fitx[i])\n",
        "            r = int(right_fitx[i])\n",
        "            cv2.line(out, (l, y), (r, y), (0, 255, 0),20)\n",
        "            if(c == 100):\n",
        "                c = 0\n",
        "                cv2.line(out,(lo,yo),(l,y), (255,0,0), 50)\n",
        "                cv2.line(out,(ro,yo),(r,y), (255,0,0), 50)\n",
        "\n",
        "        return out, out_img\n",
        "\n",
        "    def extract_features(self, img):\n",
        "        self.img = img\n",
        "        # Height of of windows - based on nwindows and image shape\n",
        "        self.window_height = np.int(img.shape[0]//self.nwindows)\n",
        "    \n",
        "        # Identify the x and y positions of all nonzero pixel in the image\n",
        "        self.nonzero = img.nonzero()\n",
        "        \n",
        "        self.nonzerox = np.array(self.nonzero[1])\n",
        "        self.nonzeroy = np.array(self.nonzero[0])\n",
        "\n",
        "    def find_lane_pixels(self, img):\n",
        "        assert(len(img.shape) == 2)\n",
        "\n",
        "        # Create an output image to draw on and visualize the result\n",
        "        out_img = np.dstack((img, img, img))\n",
        "\n",
        "        bottom_half = img[img.shape[0]//2:,:]\n",
        "        histogram =  np.sum(bottom_half, axis=0)\n",
        "\n",
        "        midpoint = histogram.shape[0]//2\n",
        "        leftx_base = np.argmax(histogram[:midpoint])\n",
        "        rightx_base = np.argmax(histogram[midpoint:]) + midpoint\n",
        "\n",
        "        # Current position to be update later for each window in nwindows\n",
        "        leftx_current = leftx_base\n",
        "        rightx_current = rightx_base\n",
        "        y_current = img.shape[0] + self.window_height//2\n",
        "\n",
        "        # Create empty lists to reveice left and right lane pixel\n",
        "        leftx, lefty, rightx, righty = [], [], [], []\n",
        "\n",
        "        # Step through the windows one by one\n",
        "        for window in range(self.nwindows):\n",
        "            # Identify window boundaries in x and y (and right and left)\n",
        "            win_y_low = img.shape[0] - (window+1)*self.window_height\n",
        "            win_y_high = img.shape[0] - window*self.window_height\n",
        "            win_xleft_low = leftx_current - self.margin\n",
        "            win_xleft_high = leftx_current +self. margin\n",
        "            win_xright_low = rightx_current - self.margin\n",
        "            win_xright_high = rightx_current + self.margin\n",
        "\n",
        "            # Draw the windows on the visualization image\n",
        "            cv2.rectangle(out_img,(win_xleft_low,win_y_low),(win_xleft_high,win_y_high),\n",
        "            (0,255,0), 2) \n",
        "            cv2.rectangle(out_img,(win_xright_low,win_y_low),(win_xright_high,win_y_high),\n",
        "            (0,255,0), 2) \n",
        "\n",
        "            y_current -= self.window_height\n",
        "            center_left = (leftx_current, y_current)\n",
        "            center_right = (rightx_current, y_current)\n",
        "\n",
        "            good_left_x, good_left_y = self.pixels_in_window(center_left, self.margin, self.window_height)\n",
        "            good_right_x, good_right_y = self.pixels_in_window(center_right, self.margin, self.window_height)\n",
        "\n",
        "            # Append these indices to the lists\n",
        "            leftx.extend(good_left_x)\n",
        "            lefty.extend(good_left_y)\n",
        "            rightx.extend(good_right_x)\n",
        "            righty.extend(good_right_y)\n",
        "\n",
        "            if len(good_left_x) > self.minpix:\n",
        "                leftx_current = np.int32(np.mean(good_left_x))\n",
        "            if len(good_right_x) > self.minpix:\n",
        "                rightx_current = np.int32(np.mean(good_right_x))\n",
        "\n",
        "        return leftx, lefty, rightx, righty, out_img\n",
        "\n",
        "    def pixels_in_window(self, center, margin, height):\n",
        "        topleft = (center[0]-margin, center[1]-height//2)\n",
        "        bottomright = (center[0]+margin, center[1]+height//2)\n",
        "    \n",
        "        condx = (topleft[0] <= self.nonzerox) & (self.nonzerox <= bottomright[0])\n",
        "        condy = (topleft[1] <= self.nonzeroy) & (self.nonzeroy <= bottomright[1])\n",
        "        return self.nonzerox[condx&condy], self.nonzeroy[condx&condy]\n",
        "\n",
        "    def measure_curvature(self):\n",
        "        ym = 30/720\n",
        "        xm = 3.7/700\n",
        "\n",
        "        left_fit = self.left_fit.copy()\n",
        "        right_fit = self.right_fit.copy()\n",
        "        y_eval = 700 * ym\n",
        "\n",
        "        # Compute R_curve (radius of curvature)\n",
        "        left_curveR =  ((1 + (2*left_fit[0] *y_eval + left_fit[1])**2)**1.5)  / np.absolute(2*left_fit[0])\n",
        "        right_curveR = ((1 + (2*right_fit[0]*y_eval + right_fit[1])**2)**1.5) / np.absolute(2*right_fit[0])\n",
        "\n",
        "        xl = np.dot(self.left_fit, [700**2, 700, 1])\n",
        "        xr = np.dot(self.right_fit, [700**2, 700, 1])\n",
        "        pos = (1280//2 - (xl+xr)//2)*xm\n",
        "        return left_curveR, right_curveR, pos"
      ],
      "metadata": {
        "id": "pvadeHSGddni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prespective Transform Class\n"
      ],
      "metadata": {
        "id": "Z-sKhZh6c5TJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PerspectiveTransformation:\n",
        "    def __init__(self):\n",
        "        \"\"\"Init PerspectiveTransformation.\"\"\"\n",
        "        self.src = np.float32([(550, 460),     # top-left\n",
        "                               (150, 720),     # bottom-left\n",
        "                               (1200, 720),    # bottom-right\n",
        "                               (770, 460)])    # top-right\n",
        "        self.dst = np.float32([(100, 0),\n",
        "                               (100, 720),\n",
        "                               (1100, 720),\n",
        "                               (1100, 0)])\n",
        "        self.M = cv2.getPerspectiveTransform(self.src, self.dst)\n",
        "        self.M_inv = cv2.getPerspectiveTransform(self.dst, self.src)\n",
        "\n",
        "    def forward(self, img, img_size=(1280, 720), flags=cv2.INTER_LINEAR):\n",
        "        return cv2.warpPerspective(img, self.M, img_size, flags=flags)\n",
        "\n",
        "    def backward(self, img, img_size=(1280, 720), flags=cv2.INTER_LINEAR):\n",
        "        return cv2.warpPerspective(img, self.M_inv, img_size, flags=flags)"
      ],
      "metadata": {
        "id": "mZbLN_IrddEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Globale Var\n"
      ],
      "metadata": {
        "id": "QGpuiQKFc9SV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "birdeye = PerspectiveTransformation()\n",
        "lanelines = LaneLines()"
      ],
      "metadata": {
        "id": "xXNmGlHOdcpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Blend Frames\n"
      ],
      "metadata": {
        "id": "mA078IXXdBLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_out_blend_frame(blend_on_road, img_binary, img_birdeye, img_fit, Rcurve, Lcurve, pos):\n",
        "    \"\"\"\n",
        "    Prepare the final pretty pretty output blend, given all intermediate pipeline images\n",
        "    :param blend_on_road: color image of lane blend onto the road\n",
        "    :param img_binary: thresholded binary image\n",
        "    :param img_birdeye: bird's eye view of the thresholded binary image\n",
        "    :param img_fit: bird's eye view with detected lane-lines highlighted\n",
        "    :param Rcurve: curve of the Right Lane\n",
        "    :param Lcurve: curve of the Left Lane\n",
        "    :param pos: offset from the center of the lane\n",
        "    :return: pretty blend with all images and stuff stitched\n",
        "    \"\"\"\n",
        "    h, w = blend_on_road.shape[:2]\n",
        "\n",
        "    thumb_ratio = 0.2\n",
        "    thumb_h, thumb_w = int(thumb_ratio * h), int(thumb_ratio * w)\n",
        "\n",
        "    off_x, off_y = 20, 15\n",
        "\n",
        "    # add a gray rectangle to highlight the upper area\n",
        "    mask = blend_on_road.copy()\n",
        "    mask = cv2.rectangle(mask, pt1=(w-(thumb_w+off_x*2), 0), pt2=(w, h), color=(0, 0, 0), thickness=cv2.FILLED)\n",
        "    blend_on_road = cv2.addWeighted(src1=mask, alpha=0.2, src2=blend_on_road, beta=0.8, gamma=0)\n",
        "\n",
        "    # add thumbnail of binary image\n",
        "    thumb_binary = cv2.resize(img_binary, dsize=(thumb_w, thumb_h))\n",
        "    # thumb_binary = np.dstack([thumb_binary, thumb_binary, thumb_binary])\n",
        "    blend_on_road[off_y:thumb_h+off_y, w-(thumb_w+off_x):w-off_x, :] = thumb_binary\n",
        "\n",
        "    # add thumbnail of bird's eye view\n",
        "    thumb_birdeye = cv2.resize(img_birdeye, dsize=(thumb_w, thumb_h))\n",
        "    thumb_birdeye = np.dstack([thumb_birdeye, thumb_birdeye, thumb_birdeye]) \n",
        "    blend_on_road[thumb_h+(2*off_y):(thumb_h*2)+(2*off_y), w-(thumb_w+off_x):w-off_x, :] = thumb_birdeye\n",
        "\n",
        "    # add thumbnail of bird's eye view (lane-line highlighted)\n",
        "    thumb_img_fit = cv2.resize(img_fit, dsize=(thumb_w, thumb_h))\n",
        "    # thumb_img_fit = np.dstack([thumb_img_fit, thumb_img_fit, thumb_img_fit])\n",
        "    blend_on_road[(thumb_h*2)+(3*off_y):(thumb_h*3)+(3*off_y), w-(thumb_w+off_x):w-off_x, :] = thumb_img_fit\n",
        "\n",
        "    # add text (curvature and offset info) on the upper right of the blend\n",
        "    mean_curvature_meter = np.mean([Lcurve, Rcurve])\n",
        "    # print(mean_curvature_meter)\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    cv2.putText(blend_on_road, 'Curvature radius: ', (w - thumb_w - off_x, (thumb_h*3)+(4*off_y)+20), font, 0.9, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "    cv2.putText(blend_on_road, '   {:.02f}m'.format(mean_curvature_meter), (w-thumb_w-off_x, (thumb_h*3)+(4*off_y)+70), font, 0.9, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "    cv2.putText(blend_on_road, 'Offset from center: ', (w-(thumb_w+off_x), (thumb_h*3)+(4*off_y)+120), font, 0.9, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "    cv2.putText(blend_on_road, '   {:.02f}m'.format(pos), (w-(thumb_w+off_x), (thumb_h*3)+(4*off_y)+170), font, 0.9, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "\n",
        "    return blend_on_road"
      ],
      "metadata": {
        "id": "DBUDEtuVdb71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Threshold methods\n"
      ],
      "metadata": {
        "id": "KAnNjoN3dGzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def threshold_rel(img, lo, hi):\n",
        "    vmin = np.min(img)\n",
        "    vmax = np.max(img)\n",
        "    \n",
        "    vlo = vmin + (vmax - vmin) * lo\n",
        "    vhi = vmin + (vmax - vmin) * hi\n",
        "    return np.uint8((img >= vlo) & (img <= vhi)) * 255\n",
        "\n",
        "def threshold_abs(img, lo, hi):\n",
        "    return np.uint8((img >= lo) & (img <= hi)) * 255"
      ],
      "metadata": {
        "id": "kDCBWg7XdbQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process Frame Pipeline\n"
      ],
      "metadata": {
        "id": "y42-o01odJkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_image(img):\n",
        "    # step 1\n",
        "    img1 = birdeye.forward(img)\n",
        "    \n",
        "    # step 2\n",
        "    hls = cv2.cvtColor(img1, cv2.COLOR_RGB2HLS)\n",
        "    hsv = cv2.cvtColor(img1, cv2.COLOR_RGB2HSV)\n",
        "    h_channel = hls[:,:,0]\n",
        "    l_channel = hls[:,:,1]\n",
        "    s_channel = hls[:,:,2]\n",
        "    v_channel = hsv[:,:,2]\n",
        "\n",
        "    right_lane = threshold_rel(l_channel, 0.8, 1.0)\n",
        "    right_lane[:,:750] = 0\n",
        "\n",
        "    left_lane = threshold_abs(h_channel, 20, 30)\n",
        "    left_lane &= threshold_rel(v_channel, 0.7, 1.0)\n",
        "    left_lane[:,550:] = 0\n",
        "\n",
        "    img2 = left_lane | right_lane\n",
        "\n",
        "    # step 3\n",
        "    img3, img4 = lanelines.forward(img2)\n",
        "    Lc, Rc, pos = lanelines.measure_curvature()\n",
        "    \n",
        "    # step 4\n",
        "    img5 = birdeye.backward(img3)\n",
        "    out_img = cv2.addWeighted(img, 1, img5, 1, 0)\n",
        "\n",
        "    out = prepare_out_blend_frame(out_img, img3, img2, img4, Lc,Rc, pos ) \n",
        "    \n",
        "    return out"
      ],
      "metadata": {
        "id": "-5zLJO9rdapg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Code\n"
      ],
      "metadata": {
        "id": "1ws29kgddVZ0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7VkvBenZEaz"
      },
      "outputs": [],
      "source": [
        "video = [\"challenge\", \"project\"]\n",
        "index = 0\n",
        "clip = VideoFileClip(\"{}_video.mp4\".format(video[index]))\n",
        "out_clip = clip.fl_image(process_image)\n",
        "out_clip.write_videofile(\"out_{}_video.mp4\".format(video[index]), audio=False)"
      ]
    }
  ]
}